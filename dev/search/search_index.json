{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Labtasker","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Labtasker is an easy-to-use task queue tool designed to manage and dispatch lab experiment tasks to user-defined workers.</p> <p>What actually is labtasker? When to use? What does it do?</p> <p>Feeling confused? Here is a quick takeaway:</p> <p>TLDR: Replace <code>for</code> loops in your experiment wrapper script (1) with labtasker to unlock a variety of powerful features (2) effortlessly.</p> <ol> <li> <p>What is a wrapper script?</p> <ul> <li>A wrapper script is not part of your experiment's core logic.</li> <li>Instead, it organizes and passes the necessary arguments to your experiment's core script.</li> <li>For example:</li> </ul> <pre><code>#!/bin/bash\n# This is a typical wrapper bash script for many ML experiments.\nfor arg1 in {0..2}; do\n    for arg2 in {3..5}; do\n        # The for loops are what you should replace with Labtasker.\n        # Only this line is the actual core logic of your experiment.\n        python train_my_model.py --arg1 $arg1 --arg2 $arg2\n    done\ndone\n</code></pre> </li> <li> <p>Labtasker provides advanced features with only 1 extra line of code:</p> <ul> <li>Load balancing and script parallelism</li> <li>Dynamic task prioritization</li> <li>Dynamic task cancellation</li> <li>Failure auto-retry and worker suspension</li> <li>Metadata recording</li> <li>And much more!</li> </ul> </li> </ol> <p>Integrating Labtasker into your existing experiment workflow requires just a few lines of boilerplate code.</p> <p>To get started, check out the quick Tutorial for an overview of the basic workflow.</p> <p>To get an overview of the motivation of this tool, continue reading.</p>"},{"location":"#motivation","title":"Motivation","text":""},{"location":"#why-not-simple-bash-wrapper-scripts","title":"Why not simple bash wrapper scripts?","text":"<p>Imagine you have multiple lab experiment jobs to run on a single GPU, such as for tasks like prompt engineering or hyperparameter search.</p> <p>The simplest approach is to write a script for each experiment and execute them sequentially.</p> run_job.sh<pre><code>#!/bin/bash\n\nfor arg1 in 1 2 3 4; do\n    for arg2 in 1 2 3 4; do\n        for arg3 in 1 2 3 4; do\n            python job_main.py --arg1 $arg1 --arg2 $arg2 --arg3 $arg3\n        done\n    done\ndone\n</code></pre> <p>This method works, but what if you have more than one worker/GPU?</p> <p>Let's say you have 4 GPUs. You would probably split the experiments into 4 groups and run them in parallel to make better use of the resources.</p> run_job_1.sh<pre><code>#!/bin/bash\n\narg1=1\nfor arg2 in 1 2 3 4; do\n    for arg3 in 1 2 3 4; do\n        python job_main.py --arg1 $arg1 --arg2 $arg2 --arg3 $arg3\n    done\ndone\n</code></pre> run_job_2.sh<pre><code>#!/bin/bash\n\narg1=2\nfor arg2 in 1 2 3 4; do\n    for arg3 in 1 2 3 4; do\n        python job_main.py --arg1 $arg1 --arg2 $arg2 --arg3 $arg3\n    done\ndone\n</code></pre> run_job_3.sh<pre><code>#!/bin/bash\n\narg1=3\nfor arg2 in 1 2 3 4; do\n    for arg3 in 1 2 3 4; do\n        python job_main.py --arg1 $arg1 --arg2 $arg2 --arg3 $arg3\n    done\ndone\n</code></pre> run_job_4.sh<pre><code>#!/bin/bash\n\narg1=4\nfor arg2 in 1 2 3 4; do\n    for arg3 in 1 2 3 4; do\n        python job_main.py --arg1 $arg1 --arg2 $arg2 --arg3 $arg3\n    done\ndone\n</code></pre> <p>However, this method can quickly become tedious and offers limited control over the experiments once the job scripts are running. Consider the following scenarios:</p> <ul> <li>How do you handle cases where the parameters are hard to divide evenly (e.g., 5x5x5 split across 3 GPUs), making it difficult to distribute the workload fairly?</li> <li>What if you realize some scheduled experiments are unnecessary after reviewing the results? (Stopping the script isn't ideal, as it would kill running jobs and make it hard to track which experiments are complete.)</li> <li>What if you want to reprioritize certain experiments based on initial results? You\u2019d face the same issue as above.</li> <li>How do you append extra experiment groups during script execution?</li> <li>What if some experiments fail midway? It can be challenging to untangle nested loops and identify completed tasks.</li> </ul> <p>Labtasker is designed to overcome these challenges.</p> <p>With Labtasker, you can submit a variety of experiment arguments to a server-based task queue. Worker nodes can then fetch and execute these tasks directly from the queue.</p>"},{"location":"#why-not-slurm","title":"Why not SLURM?","text":"<p>Unlike traditional HPC resource management systems like SLURM, Labtasker is tailored for users rather than system administrators.</p> <p>Labtasker is designed to be a simple and easy-to-use.</p> <ul> <li>It disentangles task queue from resource management.</li> <li>It offers a versatile task queue system that can be used by anyone (not just system administrators), without the need   for extensive configuration or knowledge of HPC systems.</li> </ul> <p>Here's are key conceptual differences between Labtasker and SLURM:</p> Aspects SLURM Labtasker Purpose HPC resource management system Task queue system for lab experiments Who is it for Designed for system administrators Designed for users Configuration Requires extensive configuration Minimal configuration needed Task Submission Jobs submitted as scripts with resource requirements Tasks submitted as argument groups (pythonic dictionaries) Resource Handling Allocates resources and runs the job Does not explicitly handle resource allocation Flexibility Assumes specific resource and task types No assumptions about task nature, experiment type, or computation resources Execution Runs jobs on allocated resources User-defined worker scripts run on various machines/GPUs/CPUs and decide how to handle the arguments Reporting Handled by the framework Reports results back to the server via API"},{"location":"develop/database/","title":"Database","text":"<p>Each queue identified by a unique queue_name, is responsible for managing:</p> <ol> <li>A collection of tasks (task queue)</li> <li>A collection of workers to check worker status. If a worker crashes multiple times, the tasks will be no longer be assigned to it. (worker pool)</li> <li>Authentication for the queue</li> </ol>"},{"location":"develop/database/#priority","title":"Priority","text":"<ul> <li>LOW: 0</li> <li>MEDIUM: 10  (default)</li> <li>HIGH: 20</li> </ul>"},{"location":"develop/database/#worker-fsm","title":"Worker FSM","text":"<p>states:</p> <ul> <li>active</li> <li>suspended</li> <li>crashed</li> </ul>"},{"location":"develop/database/#task-fsm","title":"Task FSM","text":"<p>states:</p> <ul> <li>created</li> <li>cancelled</li> <li>pending</li> <li>running</li> <li>success</li> <li>failed</li> </ul>"},{"location":"develop/database/#collections","title":"Collections","text":""},{"location":"develop/database/#queues-collection","title":"Queues Collection","text":"<pre><code>{\n    \"_id\": \"uuid-string\",\n    \"queue_name\": \"my_queue\",\n    \"password\": \"hashed_password\",\n    \"created_at\": \"2025-01-01T00:00:00Z\",\n    \"last_modified\": \"2025-01-01T00:00:00Z\",\n    \"metadata\": {}\n}\n</code></pre>"},{"location":"develop/database/#tasks-collection","title":"Tasks Collection","text":"<pre><code>{\n    \"_id\": \"xxxxxx\",\n    \"queue_id\": \"uuid-string\",\n    \"status\": \"created\",\n    \"task_name\": \"optional_task_name\",\n    \"created_at\": \"2025-01-01T00:00:00Z\",\n    \"start_time\": \"2025-01-01T00:00:00Z\",\n    \"last_heartbeat\": \"2025-01-01T00:00:00Z\",\n    \"last_modified\": \"2025-01-01T00:00:00Z\",\n    \"heartbeat_timeout\": 60,\n    \"task_timeout\": 3600,\n    \"max_retries\": 3,\n    \"retries\": 0,\n    \"priority\": 10,\n    \"metadata\": {},\n    \"args\": {\n        \"my_param_1\": 1,\n        \"my_param_2\": 2\n    },\n    \"cmd\": \"python main.py --arg1=1 --arg2=2\",\n    \"summary\": {},\n    \"worker_id\": \"xxxxxx\",\n}\n</code></pre>"},{"location":"develop/database/#workers-collection","title":"Workers Collection","text":"<pre><code>{\n    \"_id\": \"xxxxxx\",\n    \"queue_id\": \"uuid-string\",\n    \"status\": \"active\",\n    \"worker_name\": \"optional_worker_name\",\n    \"metadata\": {},\n    \"max_retries\": 3,\n    \"retries\": 0,\n    \"created_at\": \"2025-01-01T00:00:00Z\",\n    \"last_modified\": \"2025-01-01T00:00:00Z\"\n}\n</code></pre>"},{"location":"develop/development/","title":"Development Guide","text":""},{"location":"develop/development/#development-setup","title":"Development Setup","text":""},{"location":"develop/development/#pre-commit-hooks","title":"Pre-commit hooks","text":"<pre><code># Install pre-commit hooks for code formatting\npip install pre-commit\npre-commit install\n</code></pre>"},{"location":"develop/development/#install-development-dependencies","title":"Install development dependencies","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"develop/development/#development-utilities","title":"Development utilities","text":""},{"location":"develop/development/#format-code","title":"Format code","text":"<pre><code>make format\n</code></pre>"},{"location":"develop/development/#run-linters","title":"Run linters","text":"<pre><code>make lint\n</code></pre>"},{"location":"develop/development/#tests","title":"Tests","text":""},{"location":"develop/development/#test-setups","title":"Test setups","text":"<p>Tests are divided into unit tests, integration tests, and end-to-end tests. Some test cases are shared between unit tests, integration tests and end-to-end tests.</p> <p>Test settings</p> <p>Testcases are marked with <code>pytest.mark.unit</code>, <code>pytest.mark.integration</code>, and <code>pytest.mark.e2e</code>.</p> <p>Different tests adopts the following setting:</p> Test type Database Server &amp; Client Unit tests MongoMock TestClient &amp; ASGITransport, patched httpx client Integration tests docker mongodb service TestClient &amp; ASGITransport, patched httpx client End-to-end tests docker mongodb service docker fastapi service, httpx client to localhost"},{"location":"develop/development/#run-tests","title":"Run tests","text":"<p>Unit tests:</p> <pre><code>make unit-test\n</code></pre> <p>Do not run integration and e2e tests in production env</p> <p>Do not run integration and e2e tests in production env, as they will erase the database for testing.</p> <p>Integration tests:</p> <pre><code>make integration-test\n</code></pre> <p>End-to-end tests (quite time-consuming):</p> <pre><code>make e2e-test\n</code></pre>"},{"location":"develop/documentation/","title":"Documentation","text":""},{"location":"develop/documentation/#installation","title":"Installation","text":"<p>To install documentation dependencies:</p> <pre><code>pip install -e '.[doc]'\n</code></pre>"},{"location":"develop/documentation/#preview-locally","title":"Preview locally","text":"<p>To serve the documentation locally (for preview):</p> <pre><code>cd docs\n# make sure you are at PROJECT_ROOT/docs\n\nmike serve\n</code></pre> <p>or, you can use mkdocs to live-reload:</p> <pre><code>mkdocs serve\n</code></pre> <p>To check list of documentation versions:</p> <pre><code>make list\n</code></pre> <p>Check other utilities in <code>PROJECT_ROOT/docs/Makefile</code>.</p>"},{"location":"develop/documentation/#how-to-add-a-new-document","title":"How to add a new document","text":"<p>Steps:</p> <ol> <li>Create a new markdown file under <code>PROJECT_ROOT/docs/docs/</code> (e.g. <code>docs/docs/develop/foo.md</code>)</li> <li>Add an entry in <code>docs/mkdocs-nav.yml</code></li> </ol>"},{"location":"develop/restful-api/","title":"RESTful API","text":"<p>See implementation in <code>endpoints.py</code>.</p>"},{"location":"guide/advanced/","title":"Advanced Features","text":""},{"location":"guide/advanced/#plugins","title":"Plugins","text":""},{"location":"guide/advanced/#cli-plugins","title":"CLI plugins","text":"<p>CLI plugins are particularly useful if you want to pack up your workflow and share it with others.</p> <p>Demo plugin</p> <p>There is a demo plugin at <code>/PROJECT_ROOT/plugins/labtasker_plugin_task_count</code>.</p> <p>It creates a new custom command <code>labtasker task count</code>, which shows how many tasks are at each state.</p> <p></p> <p>To install, simply install it like a python package:</p> <pre><code>cd plugins/labtasker_plugin_task_count\npip install .\n</code></pre> <p>Note</p> <p>Behind the hood, it uses Typer command registry and setuptools entry points to implement custom CLI commands.</p> <p>To write your own CLI plugin, see Setuptools Doc and Typer Doc for details.</p>"},{"location":"guide/advanced/#workflow-plugins-wip","title":"Workflow plugins [WIP]","text":""},{"location":"guide/basic/","title":"Tutorial: Basic Workflow","text":"<p>Tip</p> <p>The code for this page is available on GitHub.</p> <p>Labtasker supports 2 sets of client APIs:</p> <ul> <li>Python: Modify your Python code with a few lines of changes to support Labtasker.</li> <li>Bash: No modification to your Python code is required. Simply wrap your command with <code>labtasker loop ...</code>.</li> </ul> <p>demo step by step</p> <p></p>"},{"location":"guide/basic/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have a deployed server.</p> <p>You can follow the Deployment guide to easily deploy a server.</p> <p>Make sure you have installed client tools.</p> <p>Following Installation.</p> <p>Make sure you have configured client.</p> <pre><code>labtasker config\n</code></pre> <p>Validate server connection.</p> <pre><code>labtasker health\n</code></pre> <p>If a task queue for current project has not been created, you can create one from the previously configured config.</p> <pre><code>labtasker queue create-from-config\n</code></pre>"},{"location":"guide/basic/#step-1-submit-job-arguments-via-python-or-cli-tool","title":"Step 1. Submit job arguments via Python or CLI tool","text":"BashPython demo/bash_demo/submit_job.sh<pre><code>#!/bin/bash\n\n# This script submits jobs with different combinations of arg1 and arg2.\n\n# Loop through arg1 and arg2 values\nfor arg1 in {0..2}; do\n    for arg2 in {3..5}; do\n        echo \"Submitting with arg1=$arg1, arg2=$arg2\"\n        labtasker task submit --args '{\"arg1\": '$arg1', \"arg2\": '$arg2'}'\n    done\ndone\n</code></pre> demo/python_demo/submit_job.py<pre><code>import labtasker\n\nif __name__ == \"__main__\":\n    for arg1 in range(3):\n        for arg2 in range(3, 6):\n            print(f\"Submitting with arg1={arg1}, arg2={arg2}\")\n            labtasker.submit_task(\n                args={\"arg1\": arg1, \"arg2\": arg2},\n            )\n</code></pre>"},{"location":"guide/basic/#step-2-run-job","title":"Step 2. Run job","text":"BashPython demo/bash_demo/run_job.sh<pre><code>#!/bin/bash\n\n# This script run jobs in loop by calling python job_main.py via labtasker loop\n# The argument can be automatically injected into the command line via %(...) syntax\nlabtasker loop -c 'python demo/bash_demo/job_main.py --arg1 %(arg1) --arg2 %(arg2)'\n</code></pre> <p>where</p> demo/bash_demo/job_main.py<pre><code>\"\"\"\nSuppose this is the given script to run certain job.\nYou would normally run this with ` python demo/bash_demo/job_main.py --arg1 1 --arg2 2`\n\"\"\"\n\nimport argparse\nimport time\n\n\ndef job(arg1: int, arg2: int):\n    \"\"\"Simulate a long-running job\"\"\"\n    time.sleep(3)  # simulate a long-running job\n    return arg1 + arg2\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--arg1\", type=int)\n    parser.add_argument(\"--arg2\", type=int)\n\n    args = parser.parse_args()\n    result = job(args.arg1, args.arg2)\n    print(f\"The result is {result}\")\n</code></pre> demo/python_demo/run_job.py<pre><code>import time\n\nimport labtasker\n\n\ndef job(arg1: int, arg2: int):\n    \"\"\"Simulate a long-running job\"\"\"\n    time.sleep(3)  # simulate a long-running job\n    return arg1 + arg2\n\n\n@labtasker.loop(required_fields=[\"arg1\", \"arg2\"])\ndef main():\n    args = labtasker.task_info().args\n    result = job(args[\"arg1\"], args[\"arg2\"])\n    print(f\"The result is {result}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"guide/basic/#check-pendingrunning-jobs","title":"Check pending/running jobs","text":"pendingrunning <pre><code>labtasker task ls --extra-filter '{\"status\": \"pending\"}'\n</code></pre> <pre><code>labtasker task ls --extra-filter '{\"status\": \"running\"}'\n</code></pre>"},{"location":"install/deployment/","title":"Deploy Server","text":"<p>TLDR</p> <p>The deployment of Labtasker is straightforward. Basically, you need to:</p> <ol> <li>Make sure repo is cloned and docker compose is installed.</li> <li>Modify the config file according to your needs.</li> <li>Start services via <code>docker compose --env-file=your_env_file.env up -d</code></li> </ol>"},{"location":"install/deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Compose</li> </ul>"},{"location":"install/deployment/#deployment","title":"Deployment","text":""},{"location":"install/deployment/#step-1-configuration","title":"Step 1: Configuration","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/fkcptlst/labtasker.git\ncd labtasker\n</code></pre></p> </li> <li> <p>Copy <code>server.example.env</code> to <code>server.env</code>:    <pre><code>cp server.example.env server.env\n</code></pre></p> </li> <li> <p>Edit <code>server.env</code> with your settings:</p> <ul> <li>Configure MongoDB.</li> <li>Configure server ports.</li> <li>Configure how often you want to check for task timeouts.</li> </ul> </li> </ol>"},{"location":"install/deployment/#step-2-start-services","title":"Step 2: Start services","text":"<ol> <li> <p>Start services:    <pre><code>docker compose --env-file server.env up -d\n</code></pre></p> </li> <li> <p>Check status:    <pre><code>docker compose --env-file server.env ps\n</code></pre></p> </li> <li> <p>View logs:    <pre><code>docker compose --env-file server.env logs -f\n</code></pre></p> </li> </ol>"},{"location":"install/deployment/#database-management","title":"Database Management","text":"<p>To expose MongoDB for external tools (this is potentially risky):</p> <ol> <li>Set <code>EXPOSE_DB=true</code> in <code>server.env</code></li> <li>Optionally set <code>DB_PORT</code> to change the exposed port (default: 27017)</li> <li>Use tools like MongoDB Compass to connect to the database.</li> </ol>"},{"location":"install/install/","title":"Installation","text":"<p>Installation</p> <p>To use Labtasker, you need to:</p> <ol> <li>Have a deployed server. See Deploy Server for more details.</li> <li>Install labtasker client tools via pip.</li> </ol> <p>The client tools is a Python package that you can install on your local machine.</p> From PyPIFrom Github <pre><code>pip install labtasker\n</code></pre> <pre><code>pip install git+https://github.com/fkcptlst/labtasker.git\n</code></pre>"}]}