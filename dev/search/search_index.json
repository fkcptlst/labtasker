{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Labtasker","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Labtasker is an easy-to-use task queue system designed for dispatching lab experiment tasks to user-defined workers.</p> <p>It enables users to submit various experiment arguments to a server-based task queue. Worker nodes can then retrieve and execute these tasks from the queue.</p> <p>Unlike traditional HPC resource management systems like SLURM, Labtasker is tailored for users rather than system administrators.</p>"},{"location":"#motivation","title":"Motivation","text":""},{"location":"#why-not-simple-bash-scripts","title":"Why not simple bash scripts?","text":"<p>Imagine you have several lab experiments to run on a single GPU, each with multiple parameters to configure.</p> <p>The simplest approach is to create a script for each experiment and execute them sequentially.</p> <pre><code>for my_param_1 in 1 2 3 4; do\n    for my_param_2 in 1 2 3 4; do\n        for my_param_3 in 1 2 3 4; do\n            python run_my_experiment.py --param1 $my_param_1 --param2 $my_param_2 --param3 $my_param_3\n        done\n    done\ndone\n</code></pre> <p>This method works, but what if you have more than one GPU?</p> <p>Let's say you have 4 GPUs. You can split the experiments into 4 groups and run them in parallel to make better use of the GPU resources.</p> <pre><code># Use my_param_1 to divide the experiments into 4 groups for 4 GPUs\n\n# my_experiment_1.sh\nmy_param_1=1\nfor my_param_2 in 1 2 3 4; do\n    for my_param_3 in 1 2 3 4; do\n        python run_my_experiment.py --param1 $my_param_1 --param2 $my_param_2 --param3 $my_param_3\n    done\ndone\n\n# my_experiment_2.sh\nmy_param_1=2\nfor my_param_2 in 1 2 3 4; do\n    for my_param_3 in 1 2 3 4; do\n        python run_my_experiment.py --param1 $my_param_1 --param2 $my_param_2 --param3 $my_param_3\n    done\ndone\n\n# my_experiment_3.sh\nmy_param_1=3\nfor my_param_2 in 1 2 3 4; do\n    for my_param_3 in 1 2 3 4; do\n        python run_my_experiment.py --param1 $my_param_1 --param2 $my_param_2 --param3 $my_param_3\n    done\ndone\n\n# my_experiment_4.sh\nmy_param_1=4\nfor my_param_2 in 1 2 3 4; do\n    for my_param_3 in 1 2 3 4; do\n        python run_my_experiment.py --param1 $my_param_1 --param2 $my_param_2 --param3 $my_param_3\n    done\ndone\n</code></pre> <p>However, this method can quickly become unwieldy and offers limited control over the experiments once the wrapper scripts are running.</p> <ul> <li>What if the parameters are difficult to divide, making it challenging to split the loop into multiple scripts?</li> <li>What if you realize some experiments are unnecessary while monitoring them live? You'd have to stop the script and modify it.</li> <li>What if you want to prioritize certain experiments after reviewing initial results? You'd have to stop the script and modify it.</li> <li>What if you want to add more experiments to the queue? You'd have to stop the script and modify it.</li> <li>What if some experiments fail? You'd need to create new scripts to restart them.</li> </ul> <p>Labtasker is designed to overcome these challenges.</p>"},{"location":"#why-not-slurm","title":"Why not SLURM?","text":"<p>Labtasker is designed to be a simple and easy-to-use.</p> <p>It disentangles task queue from resource management. It offers a versatile task queue system that can be used by anyone (not just system administrators), without the need for extensive configuration or knowledge of HPC systems.</p> <p>Here's are key conceptual differences between Labtasker and SLURM:</p> Aspects SLURM Labtasker Purpose HPC resource management system Task queue system for lab experiments Who is it for Designed for system administrators Designed for users Configuration Requires extensive configuration Minimal configuration needed Task Submission Jobs submitted as scripts with resource requirements Tasks submitted as argument groups (JSON dictionaries) Resource Handling Allocates resources and runs the job Does not explicitly handle resource allocation Flexibility Assumes specific resource and task types No assumptions about task nature, experiment type, or computation resources Execution Runs jobs on allocated resources User-defined worker scripts run on various machines/GPUs/CPUs and decide how to handle the arguments Reporting Handled by the framework Reports results back to the server via API"},{"location":"develop/database/","title":"Database","text":"<p>Each queue identified by a unique queue_name, is responsible for managing:</p> <ol> <li>A collection of tasks (task queue)</li> <li>A collection of workers to check worker status. If a worker crashes multiple times, the tasks will be no longer be assigned to it. (worker pool)</li> <li>Authentication for the queue</li> </ol>"},{"location":"develop/database/#priority","title":"Priority","text":"<ul> <li>LOW: 0</li> <li>MEDIUM: 10  (default)</li> <li>HIGH: 20</li> </ul>"},{"location":"develop/database/#worker-fsm","title":"Worker FSM","text":"<p>states:</p> <ul> <li>active</li> <li>suspended</li> <li>crashed</li> </ul>"},{"location":"develop/database/#task-fsm","title":"Task FSM","text":"<p>states:</p> <ul> <li>created</li> <li>cancelled</li> <li>pending</li> <li>running</li> <li>success</li> <li>failed</li> </ul>"},{"location":"develop/database/#collections","title":"Collections","text":""},{"location":"develop/database/#queues-collection","title":"Queues Collection","text":"<pre><code>{\n    \"_id\": \"uuid-string\",\n    \"queue_name\": \"my_queue\",\n    \"password\": \"hashed_password\",\n    \"created_at\": \"2025-01-01T00:00:00Z\",\n    \"last_modified\": \"2025-01-01T00:00:00Z\",\n    \"metadata\": {}\n}\n</code></pre>"},{"location":"develop/database/#tasks-collection","title":"Tasks Collection","text":"<pre><code>{\n    \"_id\": \"xxxxxx\",\n    \"queue_id\": \"uuid-string\",\n    \"status\": \"created\",\n    \"task_name\": \"optional_task_name\",\n    \"created_at\": \"2025-01-01T00:00:00Z\",\n    \"start_time\": \"2025-01-01T00:00:00Z\",\n    \"last_heartbeat\": \"2025-01-01T00:00:00Z\",\n    \"last_modified\": \"2025-01-01T00:00:00Z\",\n    \"heartbeat_timeout\": 60,\n    \"task_timeout\": 3600,\n    \"max_retries\": 3,\n    \"retries\": 0,\n    \"priority\": 10,\n    \"metadata\": {},\n    \"args\": {\n        \"my_param_1\": 1,\n        \"my_param_2\": 2\n    },\n    \"cmd\": \"python main.py --arg1=1 --arg2=2\",\n    \"summary\": {},\n    \"worker_id\": \"xxxxxx\",\n}\n</code></pre>"},{"location":"develop/database/#workers-collection","title":"Workers Collection","text":"<pre><code>{\n    \"_id\": \"xxxxxx\",\n    \"queue_id\": \"uuid-string\",\n    \"status\": \"active\",\n    \"worker_name\": \"optional_worker_name\",\n    \"metadata\": {},\n    \"max_retries\": 3,\n    \"retries\": 0,\n    \"created_at\": \"2025-01-01T00:00:00Z\",\n    \"last_modified\": \"2025-01-01T00:00:00Z\"\n}\n</code></pre>"},{"location":"develop/development/","title":"Development Guide","text":""},{"location":"develop/development/#development-setup","title":"Development Setup","text":""},{"location":"develop/development/#pre-commit-hooks","title":"Pre-commit hooks","text":"<pre><code># Install pre-commit hooks for code formatting\npip install pre-commit\npre-commit install\n</code></pre>"},{"location":"develop/development/#install-development-dependencies","title":"Install development dependencies","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"develop/development/#development-utilities","title":"Development utilities","text":""},{"location":"develop/development/#format-code","title":"Format code","text":"<pre><code>make format\n</code></pre>"},{"location":"develop/development/#run-linters","title":"Run linters","text":"<pre><code>make lint\n</code></pre>"},{"location":"develop/development/#tests","title":"Tests","text":""},{"location":"develop/development/#test-setups","title":"Test setups","text":"<p>Tests are divided into unit tests, integration tests, and end-to-end tests. Some test cases are shared between unit tests, integration tests and end-to-end tests.</p> <p>Test settings</p> <p>Testcases are marked with <code>pytest.mark.unit</code>, <code>pytest.mark.integration</code>, and <code>pytest.mark.e2e</code>.</p> <p>Different tests adopts the following setting:</p> Test type Database Server &amp; Client Unit tests MongoMock TestClient &amp; ASGITransport, patched httpx client Integration tests docker mongodb service TestClient &amp; ASGITransport, patched httpx client End-to-end tests docker mongodb service docker fastapi service, httpx client to localhost"},{"location":"develop/development/#run-tests","title":"Run tests","text":"<p>Unit tests:</p> <pre><code>make unit-test\n</code></pre> <p>Integration tests:</p> <pre><code>make integration-test\n</code></pre> <p>End-to-end tests (quite time-consuming):</p> <pre><code>make e2e-test\n</code></pre>"},{"location":"develop/documentation/","title":"Documentation","text":""},{"location":"develop/documentation/#installation","title":"Installation","text":"<p>To install documentation dependencies:</p> <pre><code>pip install -e '.[doc]'\n</code></pre>"},{"location":"develop/documentation/#preview-locally","title":"Preview locally","text":"<p>To serve the documentation locally (for preview):</p> <pre><code>cd docs\n# make sure you are at PROJECT_ROOT/docs\n\nmike serve\n</code></pre> <p>or, you can use mkdocs to live-reload:</p> <pre><code>mkdocs serve\n</code></pre> <p>To check list of documentation versions:</p> <pre><code>make list\n</code></pre> <p>Check other utilities in <code>PROJECT_ROOT/docs/Makefile</code>.</p>"},{"location":"develop/documentation/#how-to-add-a-new-document","title":"How to add a new document","text":"<p>Steps:</p> <ol> <li>Create a new markdown file under <code>PROJECT_ROOT/docs/docs/</code> (e.g. <code>docs/docs/develop/foo.md</code>)</li> <li>Add an entry in <code>docs/mkdocs-nav.yml</code></li> </ol>"},{"location":"develop/restful-api/","title":"RESTful API","text":"<p>See implementation in <code>endpoints.py</code>.</p>"},{"location":"guide/advanced/","title":"Advanced Features","text":""},{"location":"guide/basic/","title":"Tutorial: Basic Practices","text":""},{"location":"guide/basic/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have configured client.</p> <pre><code>labtasker config\n</code></pre> <p>Validate server connection.</p> <pre><code>labtasker health\n</code></pre> <p>If a task queue for current project has not been created, you can create one from the previously configured config.</p> <pre><code>labtasker queue create-from-config\n</code></pre>"},{"location":"guide/basic/#step-1-submit-job-arguments-via-python-or-cli-tool","title":"Step 1. Submit job arguments via Python or CLI tool","text":"BashPython demo/bash_demo/submit_job.sh<pre><code>#!/bin/bash\n\n# This script submits jobs with different combinations of arg1 and arg2.\n\n# Loop through arg1 and arg2 values\nfor arg1 in {0..2}; do\n    for arg2 in {3..5}; do\n        echo \"Submitting with arg1=$arg1, arg2=$arg2\"\n        labtasker task submit --args '{\"arg1\": '$arg1', \"arg2\": '$arg2'}'\n    done\ndone\n</code></pre> demo/python_demo/submit_job.py<pre><code>import labtasker\n\nif __name__ == \"__main__\":\n    for arg1 in range(3):\n        for arg2 in range(3, 6):\n            print(f\"Submitting with arg1={arg1}, arg2={arg2}\")\n            labtasker.submit_task(\n                args={\"arg1\": arg1, \"arg2\": arg2},\n            )\n</code></pre>"},{"location":"guide/basic/#step-2-run-job","title":"Step 2. Run job","text":"BashPython demo/bash_demo/run_job.sh<pre><code>#!/bin/bash\n\n# This script run jobs in loop by calling python job_main.py via labtasker loop\n# The argument can be automatically injected into the command line via %(...) syntax\nlabtasker loop -c 'python demo/bash_demo/job_main.py --arg1 %(arg1) --arg2 %(arg2)'\n</code></pre> <p>where</p> demo/bash_demo/job_main.py<pre><code>\"\"\"\nSuppose this is the given script to run certain job.\nYou would normally run this with ` python demo/bash_demo/job_main.py --arg1 1 --arg2 2`\n\"\"\"\n\nimport argparse\nimport time\n\n\ndef job(arg1: int, arg2: int):\n    \"\"\"Simulate a long-running job\"\"\"\n    time.sleep(3)  # simulate a long-running job\n    return arg1 + arg2\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--arg1\", type=int)\n    parser.add_argument(\"--arg2\", type=int)\n\n    args = parser.parse_args()\n    result = job(args.arg1, args.arg2)\n    print(f\"The result is {result}\")\n</code></pre> demo/python_demo/run_job.py<pre><code>import time\n\nimport labtasker\n\n\ndef job(arg1: int, arg2: int):\n    \"\"\"Simulate a long-running job\"\"\"\n    time.sleep(3)  # simulate a long-running job\n    return arg1 + arg2\n\n\n@labtasker.loop(required_fields=[\"arg1\", \"arg2\"])\ndef main():\n    args = labtasker.task_info().args\n    result = job(args[\"arg1\"], args[\"arg2\"])\n    print(f\"The result is {result}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"guide/basic/#check-pendingrunning-jobs","title":"Check pending/running jobs","text":"pendingrunning <pre><code>labtasker task ls --extra-filter '{\"status\": \"pending\"}'\n</code></pre> <pre><code>labtasker task ls --extra-filter '{\"status\": \"running\"}'\n</code></pre>"},{"location":"install/deployment/","title":"Deploy Server","text":"<p>TLDR</p> <p>The deployment of Labtasker is straightforward. Basically, you need to:</p> <ol> <li>Make sure repo is cloned and docker compose is installed.</li> <li>Modify the config file according to your needs.</li> <li>Start services via <code>docker compose --env-file=your_env_file.env up -d</code></li> </ol>"},{"location":"install/deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Compose</li> </ul>"},{"location":"install/deployment/#deployment","title":"Deployment","text":""},{"location":"install/deployment/#step-1-configuration","title":"Step 1: Configuration","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/fkcptlst/labtasker.git\ncd labtasker\n</code></pre></p> </li> <li> <p>Copy <code>server.example.env</code> to <code>server.env</code>:    <pre><code>cp server.example.env server.env\n</code></pre></p> </li> <li> <p>Edit <code>server.env</code> with your settings:</p> <ul> <li>Configure MongoDB.</li> <li>Configure server ports.</li> <li>Configure how often you want to check for task timeouts.</li> </ul> </li> </ol>"},{"location":"install/deployment/#step-2-start-services","title":"Step 2: Start services","text":"<ol> <li> <p>Start services:    <pre><code>docker compose --env-file server.env up -d\n</code></pre></p> </li> <li> <p>Check status:    <pre><code>docker compose --env-file server.env ps\n</code></pre></p> </li> <li> <p>View logs:    <pre><code>docker compose --env-file server.env logs -f\n</code></pre></p> </li> </ol>"},{"location":"install/deployment/#database-management","title":"Database Management","text":"<p>To expose MongoDB for external tools (this is potentially risky):</p> <ol> <li>Set <code>EXPOSE_DB=true</code> in <code>server.env</code></li> <li>Optionally set <code>DB_PORT</code> to change the exposed port (default: 27017)</li> <li>Use tools like MongoDB Compass to connect to the database.</li> </ol>"},{"location":"install/install/","title":"Installation","text":"<p>Installation</p> <p>To use Labtasker, you need to:</p> <ol> <li>Have a deployed server. See Deploy Server for more details.</li> <li>Install labtasker client tools via pip.</li> </ol> <p>The client tools is a Python package that you can install on your local machine.</p> From PyPIFrom Github <pre><code># TODO: to be updated\npip install labtasker\n</code></pre> <pre><code>pip install git+https://github.com/fkcptlst/labtasker.git\n</code></pre>"}]}