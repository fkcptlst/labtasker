{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Labtasker","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Labtasker is an easy-to-use task queue system designed for dispatching lab experiment tasks to user-defined workers.</p> <p>It enables users to submit various experiment arguments to a server-based task queue. Worker nodes can then retrieve and execute these tasks from the queue.</p> <p>Unlike traditional HPC resource management systems like SLURM, Labtasker is tailored for users rather than system administrators.</p>"},{"location":"#motivation","title":"Motivation","text":""},{"location":"#why-not-simple-bash-scripts","title":"Why not simple bash scripts?","text":"<p>Imagine you have several lab experiments to run on a single GPU, each with multiple parameters to configure.</p> <p>The simplest approach is to create a script for each experiment and execute them sequentially.</p> <pre><code>for my_param_1 in 1 2 3 4; do\n    for my_param_2 in 1 2 3 4; do\n        for my_param_3 in 1 2 3 4; do\n            python run_my_experiment.py --param1 $my_param_1 --param2 $my_param_2 --param3 $my_param_3\n        done\n    done\ndone\n</code></pre> <p>This method works, but what if you have more than one GPU?</p> <p>Let's say you have 4 GPUs. You can split the experiments into 4 groups and run them in parallel to make better use of the GPU resources.</p> <pre><code># Use my_param_1 to divide the experiments into 4 groups for 4 GPUs\n\n# my_experiment_1.sh\nmy_param_1=1\nfor my_param_2 in 1 2 3 4; do\n    for my_param_3 in 1 2 3 4; do\n        python run_my_experiment.py --param1 $my_param_1 --param2 $my_param_2 --param3 $my_param_3\n    done\ndone\n\n# my_experiment_2.sh\nmy_param_1=2\nfor my_param_2 in 1 2 3 4; do\n    for my_param_3 in 1 2 3 4; do\n        python run_my_experiment.py --param1 $my_param_1 --param2 $my_param_2 --param3 $my_param_3\n    done\ndone\n\n# my_experiment_3.sh\nmy_param_1=3\nfor my_param_2 in 1 2 3 4; do\n    for my_param_3 in 1 2 3 4; do\n        python run_my_experiment.py --param1 $my_param_1 --param2 $my_param_2 --param3 $my_param_3\n    done\ndone\n\n# my_experiment_4.sh\nmy_param_1=4\nfor my_param_2 in 1 2 3 4; do\n    for my_param_3 in 1 2 3 4; do\n        python run_my_experiment.py --param1 $my_param_1 --param2 $my_param_2 --param3 $my_param_3\n    done\ndone\n</code></pre> <p>However, this method can quickly become unwieldy and offers limited control over the experiments once the wrapper scripts are running.</p> <ul> <li>What if the parameters are difficult to divide, making it challenging to split the loop into multiple scripts?</li> <li>What if you realize some experiments are unnecessary while monitoring them live? You'd have to stop the script and modify it.</li> <li>What if you want to prioritize certain experiments after reviewing initial results? You'd have to stop the script and modify it.</li> <li>What if you want to add more experiments to the queue? You'd have to stop the script and modify it.</li> <li>What if some experiments fail? You'd need to create new scripts to restart them.</li> </ul> <p>Labtasker is designed to overcome these challenges.</p>"},{"location":"#why-not-slurm","title":"Why not SLURM?","text":"<p>Labtasker is designed to be a simple and easy-to-use.</p> <p>It disentangles task queue from resource management. It offers a versatile task queue system that can be used by anyone (not just system administrators), without the need for extensive configuration or knowledge of HPC systems.</p> <p>Here's are key conceptual differences between Labtasker and SLURM:</p> Aspects SLURM Labtasker Purpose HPC resource management system Task queue system for lab experiments Who is it for Designed for system administrators Designed for users Configuration Requires extensive configuration Minimal configuration needed Task Submission Jobs submitted as scripts with resource requirements Tasks submitted as argument groups (JSON dictionaries) Resource Handling Allocates resources and runs the job Does not explicitly handle resource allocation Flexibility Assumes specific resource and task types No assumptions about task nature, experiment type, or computation resources Execution Runs jobs on allocated resources User-defined worker scripts run on various machines/GPUs/CPUs and decide how to handle the arguments Reporting Handled by the framework Reports results back to the server via API"},{"location":"best-practices/advanced/","title":"Advanced Features","text":""},{"location":"best-practices/advanced/#database-management","title":"Database management","text":"<p>The backend uses MongoDB to store task data. The server itself does not preserve any state or data. This allows you to directly access the database using a database management tool to navigate and manage the tasks.</p>"},{"location":"best-practices/advanced/#queue-management","title":"Queue management","text":""},{"location":"best-practices/advanced/#create-queue","title":"Create queue","text":"<pre><code>labtasker create-queue --client-config ./my_client_config.env --queue-name my_queue_name\n</code></pre>"},{"location":"best-practices/advanced/#task-management","title":"Task management","text":""},{"location":"best-practices/advanced/#getting-task-list-no-pop","title":"Getting task list (no pop)","text":"<pre><code># get a single task\nlabtasker ls-tasks --client-config ./my_client_config.env --task-id my_task_id\n\n# get tasks by name\nlabtasker ls-tasks --client-config ./my_client_config.env --task-name my_task_name\n\n# get all tasks in a queue, specified in my_client_config.env\nlabtasker ls-tasks --client-config ./my_client_config.env\n</code></pre>"},{"location":"best-practices/advanced/#adding-metadata-to-tasks","title":"Adding metadata to tasks","text":"<p>When submitting tasks, you can add metadata to the task. Metadata is a separate field from <code>args</code>. This allows for flexible task management and reporting.</p> <p>Example: Tags implemented by metadata</p> <p>You can implement tag feature using metadata.</p> PythonBash <pre><code>import labtasker\n\ntasker = labtasker.LabtaskerClient(client_config=\"./my_client_config.env\")\n\ntasker.submit(\n    task_name=\"optional_task_name\",\n    args={\"my_param_1\": my_param_1, \"my_param_2\": my_param_2},\n    metadata={\"tags\": [\"my_tag_1\", \"my_tag_2\"]}\n)\n</code></pre> <pre><code>labtasker submit --client-config ./my_client_config.env \\\n--task-name optional_task_name \\\n--metadata '{\"tags\": [\"my_tag_1\", \"my_tag_2\"]}' \\\n--args '{\"my_param_1\": $my_param_1, \"my_param_2\": $my_param_2}'\n</code></pre>"},{"location":"best-practices/advanced/#task-settings","title":"Task settings","text":""},{"location":"best-practices/advanced/#priority","title":"Priority","text":""},{"location":"best-practices/advanced/#timeout","title":"Timeout","text":""},{"location":"best-practices/advanced/#retries","title":"Retries","text":""},{"location":"best-practices/advanced/#task-filtering","title":"Task filtering","text":""},{"location":"best-practices/advanced/#worker-management","title":"Worker management","text":""},{"location":"best-practices/advanced/#suspend-worker","title":"Suspend worker","text":""},{"location":"best-practices/advanced/#daemon-in-bash-api","title":"Daemon in bash API","text":""},{"location":"best-practices/advanced/#heartbeat-via-bash","title":"Heartbeat via bash","text":""},{"location":"best-practices/basic/","title":"Basic Practices","text":"<p>This section describes the basic practices for using Labtasker.</p> <p>After installation, you can use Labtasker to schedule your experiment tasks.</p>"},{"location":"best-practices/basic/#basic-workflow","title":"Basic workflow","text":"<ol> <li>Submit tasks to the server via a bash script, looping over a list of arguments.</li> <li>Start multiple worker scripts, each using Labtasker client API to fetch tasks from the server.</li> <li>The worker scripts execute the tasks and report the results back to the server.</li> </ol> <p>Suppose your server is running on <code>localhost:8080</code>.</p> <p>Note</p> <p>The following \"Python\" and \"Bash\" API can mix. That is, you can submit tasks via Python script and execute tasks via Bash script.</p>"},{"location":"best-practices/basic/#step-0-client-configuration","title":"Step 0. Client configuration","text":"<p>You can start interactive cli to create a client configuration file.</p> <pre><code>labtasker config\n</code></pre> <p>You can see the example of the client configuration file in <code>client.example.env</code>.</p> <pre><code># my_client_config.env\n\nHTTP_SERVER_ADDRESS=localhost:8080\n\nQUEUE_NAME=my_queue\nPASSWORD=my_password  # a password for a queue\n</code></pre>"},{"location":"best-practices/basic/#step-1-create-a-task-queue","title":"Step 1. Create a task queue","text":"<p>First, you need to create a task queue.</p> <p>The following command loads the client configuration from <code>my_client_config.env</code> and creates a task queue with name and password specified in the client configuration.</p> Option 1. PythonOption 2. Bash <pre><code>import labtasker\n\ntasker = labtasker.LabtaskerClient(client_config=\"./my_client_config.env\")\nstatus, queue_id = tasker.create_queue()\n</code></pre> <pre><code>labtasker create-queue --client-config ./my_client_config.env\n\n# {\n#     \"status\": \"success\",\n#     \"queue_name\": \"my_queue\",\n#     \"queue_id\": \"xxxxxx\"\n# }\n</code></pre>"},{"location":"best-practices/basic/#step-2-submit-tasks","title":"Step 2. Submit tasks","text":"Option 1. PythonOption 2. Bash <pre><code># submit_tasks.py\nimport labtasker\n\ntasker = labtasker.LabtaskerClient(client_config=\"./my_client_config.env\")\n\nfor my_param_1 in range(10):\n    for my_param_2 in range(10):\n        tasker.submit(task_name=\"optional_task_name\", args={\"my_param_1\": my_param_1, \"my_param_2\": my_param_2})\n</code></pre> <pre><code># submit_tasks.sh\n#\n# submit task parameters as a JSON string, loop over different combinations of task parameters\n# {\n#     \"my_param_1\": 1,\n#     \"my_param_2\": 2\n# }\nfor my_param_1 in {1..10}\ndo\n    for my_param_2 in {1..10}\n    do\n        labtasker submit --client-config ./my_client_config.env --task-name optional_task_name --args '{\"my_param_1\": $my_param_1, \"my_param_2\": $my_param_2}'\n        # returns a JSON string containing relevant task information\n    done\ndone\n</code></pre>"},{"location":"best-practices/basic/#step-3-execute-tasks","title":"Step 3. Execute tasks","text":"Option 1. PythonOption 2. Bash <p>This approach is intrusive to the task Python script</p> <pre><code># task_runner.py\nimport time\nimport labtasker\nfrom subprocess import run\n\ntasker = labtasker.LabtaskerClient(client_config=\"./my_client_config.env\")\n\nwhile True:\n    task = tasker.fetch(eta_max=\"2h\", start_heartbeat=True)\n    if task.status == \"empty\":\n        break\n    elif task.status == \"error\":\n        continue\n\n# Execute the task\n    try:\n        run([\"python\", \"run_my_experiment.py\", \"--param1\", task.args[\"my_param_1\"], \"--param2\", task.args[\"my_param_2\"]])\n    except Exception as e:\n        task.report(status=\"failure\", summary={\"error\": str(e)})\n        continue\n\ntask.report(status=\"success\", summary={\"log_file_path\": \"/path/to/log/file\"})\n\ntime.sleep(1)\n</code></pre> <p>This approach is non-intrusive to the task Python script</p> <pre><code># work.sh\n\n#perhaps?\n\nlabtasker run -c 'python run_my_experiment.py --param1 ${ltc.my_param_1} --param2 ${ltc.my_param_2}'\n\nCONFIG_PATH=\"./my_client_config.env\"\n\nwhile true\ndo\n    # Fetch task arguments\n    response=$(labtasker fetch --client-config \"$CONFIG_PATH\" --eta-max 2h)  # the timeout for single task execution is 2 hours. Required for bash cli, since heartbeat is not supported for bash cli.\n\n# Extract status from the JSON response, one of \"empty\", \"error\", \"success\"\n    status=$(echo \"$response\" | jq -r '.status')\n\ncase \"$status\" in\n        \"empty\")\n            echo \"No tasks to execute\"\n            break\n            ;;\n        \"error\")\n            echo \"Task fetch failed\"\n            sleep 10\n            continue\n            ;;\n        \"success\")\n            echo \"Task fetched successfully\"\n            # Proceed with task execution\n            ;;\n        *)\n            echo \"Unexpected status: $status\"\n            sleep 10\n            continue\n            ;;\n    esac\n\n# Extract task ID and parameters from nested JSON\n    # {\n    #     \"status\": \"success\",\n    #     \"task_id\": \"123\",\n    #     \"args\": {\n    #         \"my_param_1\": \"1\",\n    #         \"my_param_2\": \"2\"\n    #     }\n    # }\n    task_id=$(echo \"$response\" | jq -r '.task_id')\n    my_param_1=$(echo \"$response\" | jq -r '.args.my_param_1')\n    my_param_2=$(echo \"$response\" | jq -r '.args.my_param_2')\n\n# Execute the task\n    if python run_my_experiment.py --param1 \"$my_param_1\" --param2 \"$my_param_2\"; then\n        # Report success\n        labtasker report --client-config \"$CONFIG_PATH\" --task-id \"$task_id\" --status \"success\" --summary '{\"log_file_path\": \"/path/to/log/file\"}'\n    else\n        # Report failure\n        labtasker report --client-config \"$CONFIG_PATH\" --task-id \"$task_id\" --status \"failed\" --summary '{\"log_file_path\": \"/path/to/log/file\"}'\n    fi\n\n# Optional: Sleep to avoid tight loop\n    sleep 1\ndone\n</code></pre>"},{"location":"best-practices/basic/#feature-comparison","title":"Feature comparison","text":"<p>We compare the Python and Bash API:</p> Features Python Bash Heartbeat \u2705 - Worker-ID auto-register \u2705 - Error worker auto-suspend \u2705 - Create queue \u2705 \u2705 Submit \u2705 \u2705 Upload metadata \u2705 \u2705 Task filtering \u2705 \u2705 <p>Why are some features not supported in Bash?</p> <p>The fundamental reason is that bash is difficult to implement a daemon process that automatically sends heartbeat, maintains a worker ID and quits when the bash script exits.</p> <p>Such functionality can be achieved via <code>TRAP</code> mechanism in bash. But it increases the complexity of the bash script.</p> <p>We show the example in the advanced use cases section.</p>"},{"location":"develop/database/","title":"Database","text":"<p>Each queue indentified by a unique queue_name, is responsible for managing:</p> <ol> <li>A collection of tasks (task queue)</li> <li>A collection of workers to check worker status. If a worker crashes multiple times, the tasks will be no longer be assigned to it. (worker pool)</li> <li>Authentication for the queue</li> </ol>"},{"location":"develop/database/#priority","title":"Priority","text":"<ul> <li>LOW: 0</li> <li>MEDIUM: 10  (default)</li> <li>HIGH: 20</li> </ul>"},{"location":"develop/database/#worker-fsm","title":"Worker FSM","text":"<p>states:</p> <ul> <li>active</li> <li>suspended</li> <li>crashed</li> </ul>"},{"location":"develop/database/#task-fsm","title":"Task FSM","text":"<p>states:</p> <ul> <li>created</li> <li>cancelled</li> <li>pending</li> <li>running</li> <li>completed</li> <li>failed</li> </ul>"},{"location":"develop/database/#collections","title":"Collections","text":""},{"location":"develop/database/#queues-collection","title":"Queues Collection","text":"<pre><code>{\n    \"_id\": \"uuid-string\",\n    \"queue_name\": \"my_queue\",\n    \"password\": \"hashed_password\",\n    \"created_at\": \"2025-01-01T00:00:00Z\",\n    \"last_modified\": \"2025-01-01T00:00:00Z\",\n    \"metadata\": {}\n}\n</code></pre> <ul> <li>Create: \u2705</li> <li>Delete: \u2705</li> <li>Update: \u2705</li> </ul>"},{"location":"develop/database/#tasks-collection","title":"Tasks Collection","text":"<pre><code>{\n    \"_id\": \"xxxxxx\",\n    \"queue_id\": \"uuid-string\",\n    \"status\": \"created\",\n    \"task_name\": \"optional_task_name\",\n    \"created_at\": \"2025-01-01T00:00:00Z\",\n    \"start_time\": \"2025-01-01T00:00:00Z\",\n    \"last_heartbeat\": \"2025-01-01T00:00:00Z\",\n    \"last_modified\": \"2025-01-01T00:00:00Z\",\n    \"heartbeat_timeout\": 60,\n    \"task_timeout\": 3600,\n    \"max_retries\": 3,\n    \"retries\": 0,\n    \"priority\": 10,\n    \"metadata\": {},\n    \"args\": {\n        \"my_param_1\": 1,\n        \"my_param_2\": 2\n    },\n    \"cmd\": \"python main.py --arg1=1 --arg2=2\",\n    \"summary\": {},\n    \"worker_id\": \"xxxxxx\",\n}\n</code></pre> <ul> <li>Create: \u2705</li> <li>Update: \u2705</li> <li>Delete: \u2705</li> </ul>"},{"location":"develop/database/#workers-collection","title":"Workers Collection","text":"<pre><code>{\n    \"_id\": \"xxxxxx\",\n    \"queue_id\": \"uuid-string\",\n    \"status\": \"active\",\n    \"worker_name\": \"optional_worker_name\",\n    \"metadata\": {},\n    \"max_retries\": 3,\n    \"retries\": 0,\n    \"created_at\": \"2025-01-01T00:00:00Z\",\n    \"last_modified\": \"2025-01-01T00:00:00Z\"\n}\n</code></pre> <ul> <li>Create: \u2705</li> <li>Delete: \u2705</li> <li>Update: \u2705</li> </ul>"},{"location":"develop/deployment/","title":"Deployment Guide","text":""},{"location":"develop/deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>Docker Compose</li> </ul>"},{"location":"develop/deployment/#configuration","title":"Configuration","text":"<ol> <li> <p>Copy <code>server.example.env</code> to <code>server.env</code>:    <pre><code>cp server.example.env server.env\n</code></pre></p> </li> <li> <p>Edit <code>server.env</code> with your settings:</p> </li> <li>Set secure passwords</li> <li>Configure ports</li> <li>Set security pepper</li> </ol>"},{"location":"develop/deployment/#database-management","title":"Database Management","text":"<p>To expose MongoDB for external tools: 1. Set <code>EXPOSE_DB=true</code> in <code>server.env</code> 2. Optionally set <code>DB_PORT</code> to change the exposed port (default: 27017) 2. Use tools like MongoDB Compass with:    <pre><code>mongodb://username:password@localhost:27017\n</code></pre></p>"},{"location":"develop/deployment/#deployment","title":"Deployment","text":"<ol> <li> <p>Start services:    <pre><code>docker-compose --env-file server.env up -d\n</code></pre></p> </li> <li> <p>Check status:    <pre><code>docker-compose ps\n</code></pre></p> </li> <li> <p>View logs:    <pre><code>docker-compose logs -f\n</code></pre></p> </li> </ol>"},{"location":"develop/deployment/#maintenance","title":"Maintenance","text":"<ul> <li> <p>Backup database:   <pre><code>docker exec labtasker-mongodb mongodump --out /data/backup\n</code></pre></p> </li> <li> <p>Restore database:   <pre><code>docker exec labtasker-mongodb mongorestore /data/backup\n</code></pre></p> </li> </ul>"},{"location":"develop/development/","title":"Development Guide","text":""},{"location":"develop/development/#development-setup","title":"Development Setup","text":""},{"location":"develop/development/#pre-commit-hooks","title":"Pre-commit hooks","text":"<pre><code># Install pre-commit hooks\npip install pre-commit\npre-commit install\n</code></pre>"},{"location":"develop/development/#install-development-dependencies","title":"Install development dependencies","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"develop/development/#format-code","title":"Format code","text":"<pre><code>make format\n</code></pre>"},{"location":"develop/development/#run-linters","title":"Run linters","text":"<pre><code>make lint\n</code></pre>"},{"location":"develop/development/#run-tests","title":"Run tests","text":"<pre><code>make test\n</code></pre>"},{"location":"develop/development/#documentation","title":"Documentation","text":"<pre><code>cd docs\nmike serve\n# or use mkdocs to live-reload\nmkdocs serve\n</code></pre> <p>Check list of versions:</p> <pre><code>make list\n</code></pre> <p>Check other utilities in <code>Makefile</code>.</p>"},{"location":"develop/restful-api/","title":"RESTful API","text":""},{"location":"install/install/","title":"Installation","text":"<p>To use Labtasker, you need to deploy a server and install client on your worker machines.</p> <p>The deployment of Labtasker is straightforward. Basically, you need to:</p> <ol> <li>Deploy Labtasker server (e.g. on a cloud server).</li> <li>Install Labtasker Python client on your local machine.</li> </ol>"},{"location":"install/install/#deploy-server","title":"Deploy server","text":"<p>You can deploy Labtasker server on a cloud server via docker-compose.</p> <pre><code>git clone https://github.com/fkcptlst/labtasker.git\ncd labtasker\n</code></pre> <p>Copy the <code>server.example.env</code> file to <code>server.env</code> and change the environment variables to your own values.</p> <pre><code>cp server.example.env server.env\n\n# Edit the server.env file\n# ...\n</code></pre> <p>Then, run the following command to start the server:</p> <pre><code>docker-compose --env-file server.env up -d\n</code></pre>"},{"location":"install/install/#install-client","title":"Install client","text":"<p>The client is a Python package that you can install on your local machine.</p> <pre><code>pip install labtasker\n</code></pre>"}]}